{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tensorflow \n",
    "===\n",
    "A language in a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a great framework capable of providing an incredible abstraction for complex algebra and deep learning.\n",
    "\n",
    "Tensorflow is a a part of keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Symbolic variables\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders/ Constants "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has it's own variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x1 = tf.constant(3.0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x2 = tf.constant(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print just shows two nodes to display the content of the nodes we need to evaluate them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to start a session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([x1, x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(x3):  7.0\n"
     ]
    }
   ],
   "source": [
    "x3 = tf.add(x1, x2)\n",
    "print(\"x3: \", x3)\n",
    "print(\"sess.run(x3): \",sess.run(x3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b:4.5}))\n",
    "print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b:4.5}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Tensorflow can change variables in contrast to constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "\n",
    "# data is put into X\n",
    "# Tensorflow is expected to learn W and b\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function initializes all variables from above. \n",
    "# If you do not define a value for the variables, Tensorflow assigns smal random numbers to the function.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "# New placeholder y for true outputs\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Defintion of loss function: square (tf.square) of the linear model (linear_model) and the true value (y)\n",
    "\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "\n",
    "# Now we sum of squared deltas (of the squared residuals)\n",
    "\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "# Now we compute the loss for x and y\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Change manually the values of W and b:\n",
    "\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "\n",
    "# initialize W and b\n",
    "sess.run([fixW, fixb])\n",
    "\n",
    "# run the loss function again\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer is a function of tensorflow (Stochastic gradient Descent Optimizer, see slides)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "# Apply the optimizer to the loss function\n",
    "train = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the train-function 1000 times\n",
    "\n",
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Now print the optimal weights according to the optimizer\n",
    "\n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Otto Group is one of the worldâ€™s biggest e-commerce companies, A consistent analysis of the performance of products is crucial. However, due to diverse global infrastructure, many identical products get classified differently.\n",
    "For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. \n",
    "Each row corresponds to a single product. There are a total of 93 numerical features, which represent counts of different events. All features have been obfuscated and will not be defined any further.\n",
    "\n",
    "https://www.kaggle.com/c/otto-group-product-classification-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, train=True):\n",
    "#def load_data(~bozhinvi/Dropbox/my_python_scripts/Part_2/, train=True):\n",
    "\n",
    "    \"\"\"Load data from a CSV File\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        The path to the CSV file\n",
    "        \n",
    "    train: bool (default True)\n",
    "        Decide whether or not data are *training data*.\n",
    "        If True, some random shuffling is applied.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    X: numpy.ndarray \n",
    "        The data as a multi dimensional array of floats\n",
    "    ids: numpy.ndarray\n",
    "        A vector of ids for each sample\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    if train:\n",
    "        np.random.shuffle(X)  # https://youtu.be/uyUXoap67N8\n",
    "        X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "        return X, labels\n",
    "    else:\n",
    "        X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "        return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling data to mean 0, Std. Dev. 1\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    \"\"\"Preprocess input data by standardise features \n",
    "    by removing the mean and scaling to unit variance\"\"\"\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler\n",
    "\n",
    "# Recodes labels to numbes\n",
    "\n",
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    \"\"\"Encode labels with values among 0 and `n-classes-1`\"\"\"\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 classes\n",
      "93 dims\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_full, labels = load_data('data/otto/train.csv', train=True)\n",
    "\n",
    "# Preprocess data\n",
    "X_full, scaler = preprocess_data(X_full)\n",
    "\n",
    "# Preprocess the labels\n",
    "Y_full, encoder = preprocess_labels(labels)\n",
    "\n",
    "X_full, _ = preprocess_data(X_full, scaler)\n",
    "\n",
    "# Split data to train and test data and \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_full, Y_full, test_size=0.33, random_state=42)\n",
    "\n",
    "# Gives back the dimensionality of Y_train (col\n",
    "nb_classes = Y_train.shape[1]\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "# Gives back the dimensionality of X_train -> Varable number\n",
    "dims = X_train.shape[1]\n",
    "print(dims, 'dims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create and train a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands On - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.003332357\n",
      "Epoch: 0002 cost= 0.002992140\n",
      "Epoch: 0003 cost= 0.002793532\n",
      "Epoch: 0004 cost= 0.002655072\n",
      "Epoch: 0005 cost= 0.002550682\n",
      "Epoch: 0006 cost= 0.002468359\n",
      "Epoch: 0007 cost= 0.002401410\n",
      "Epoch: 0008 cost= 0.002345693\n",
      "Epoch: 0009 cost= 0.002298490\n",
      "Epoch: 0010 cost= 0.002257920\n",
      "Epoch: 0011 cost= 0.002222643\n",
      "Epoch: 0012 cost= 0.002191660\n",
      "Epoch: 0013 cost= 0.002164216\n",
      "Epoch: 0014 cost= 0.002139720\n",
      "Epoch: 0015 cost= 0.002117706\n",
      "Epoch: 0016 cost= 0.002097795\n",
      "Epoch: 0017 cost= 0.002079684\n",
      "Epoch: 0018 cost= 0.002063121\n",
      "Epoch: 0019 cost= 0.002047899\n",
      "Epoch: 0020 cost= 0.002033845\n",
      "Epoch: 0021 cost= 0.002020817\n",
      "Epoch: 0022 cost= 0.002008693\n",
      "Epoch: 0023 cost= 0.001997371\n",
      "Epoch: 0024 cost= 0.001986764\n",
      "Epoch: 0025 cost= 0.001976797\n",
      "Optimization Finished!\n",
      "Accuracy: 0.725367\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01 # Step we do everytime we do a stochastic gradient descent\n",
    "training_epochs = 25 # Number of times, the gradients are passed / iteration number\n",
    "batch_size = 100     # Mini batch, 100 points for which the gradient is computed and averaged\n",
    "display_step = 1     # Gives information of the progress\n",
    "\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 93]) #93 dim\n",
    "y = tf.placeholder(tf.float32, [None, 9]) # 9 classes\n",
    "\n",
    "# Set model weights: due to the randomization, the result might differ a bit on different machines. Moreover,\n",
    "# different machines yield slightly different results due to CPU/GPU characteristics.\n",
    "W = tf.Variable(tf.zeros([93, 9])) # Setting model parameters: weights, initialized with small random numbers\n",
    "b = tf.Variable(tf.zeros([9]))     # Setting model parameters: betas, initialized with small random numbers\n",
    "\n",
    "# y = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "#\n",
    "# Inbetween we have a weight matrix with 93x9 dimension mapping from x to y -> y = Wx\n",
    "#\n",
    "# x = [1, 4, 5, ... , 8] (length = 93)\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax, see Slide 33\n",
    "\n",
    "# Minimize error using cross entropy: mean of the sum of the logistic regression\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "# Gradient Descent minimizes cost function (cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Monitors the decrease of the loss function\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        # Loop over all batches: batch of dataset is passed\n",
    "        for start, end in zip(range(0, len(X_train), batch_size), range(batch_size, len(X_train), batch_size)):\n",
    "            _,c = sess.run([optimizer, cost], feed_dict={x: X_train[start:end], y: Y_train[start:end]})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += c / (X_train.shape[0]/batch_size)\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
